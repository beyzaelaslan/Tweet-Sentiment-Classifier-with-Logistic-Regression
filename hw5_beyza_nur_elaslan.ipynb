{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUj26hXagbIP"
      },
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/y-akbal/ADA440_Python_4_DS/blob/main/ALE/tweet_classifier.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSPOrXGygbIT"
      },
      "source": [
        "# Tweet Sentiment Classifier with Logistic Regression\n",
        "This week you are on your own, you will learn about logistic regression. Given a tweet, you will decide if it has a positive sentiment or a negative one. Specifically you will:\n",
        "\n",
        "* Learn how to extract features for logistic regression given some text using bag of words model,\n",
        "* Implement logistic regression from scratch,\n",
        "\n",
        "We will be using a data set of tweets given in csv form.\n",
        "\n",
        "Things you shall be doing:\n",
        "\n",
        "* Implement logistic regression from scratch,\n",
        "* Read the csv file and extract features,\n",
        "* Train your model on your dataset,\n",
        "* Do some funny stuff.\n",
        "\n",
        "Before you get started, please see\n",
        "https://en.wikipedia.org/wiki/Logistic_regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lEfMcis9gbIU"
      },
      "outputs": [],
      "source": [
        "##Import some libraries!!!\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vX7jQoAugbIW"
      },
      "source": [
        "# Part 1: Logistic regression\n",
        "\n",
        "\n",
        "### Part 1.1: Sigmoid\n",
        "You will learn to use logistic regression for text classification.\n",
        "* The sigmoid function is defined as:\n",
        "\n",
        "$$ \\sigma(z) = \\frac{1}{1+e^{-z}} \\tag{1}$$\n",
        "\n",
        "It maps the input 'z' to a value that ranges between 0 and 1, and so it can be treated as a probability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WyhCymIPgbIW"
      },
      "outputs": [],
      "source": [
        "##Implement sigmoid function now!!!\n",
        "def sigmoid(x:np.ndarray)->np.ndarray:\n",
        "    return 1 / (1 + np.exp(-x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vLg8hmiMgbIX",
        "outputId": "97f13f4e-13fe-4900-f255-5f6fb5965fe4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Great Successs!!!\n",
            "Good!!!\n"
          ]
        }
      ],
      "source": [
        "# Testing your function\n",
        "if (sigmoid(0) == 0.5):\n",
        "    print('Great Successs!!!')\n",
        "else:\n",
        "    print('Oops!')\n",
        "\n",
        "if (sigmoid(4.92) == 0.9927537604041685):\n",
        "    print('Good!!!')\n",
        "else:\n",
        "    print('Oops you did it again!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sCZOFMdgbIX"
      },
      "source": [
        "### Part 1.2: Logistic Regression Class\n",
        "See the previous week (linear regression) for a quick refresher. The set up is the same.\n",
        "### Logistic regression: regression and a sigmoid\n",
        "\n",
        "Logistic regression takes a regular linear regression, and applies a sigmoid to the output of the linear regression.\n",
        "\n",
        "Regression:\n",
        "$$z = \\theta_0  + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n",
        "Note that the $\\theta$ values are \"weights\".\n",
        "Logistic regression\n",
        "$$ \\sigma(z) = \\frac{1}{1+e^{-z}}$$\n",
        "$$z = \\theta_0  + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n",
        "We will refer to 'z' as the 'logits'.\n",
        "\n",
        "### Part 1.2 Cost function and Gradient\n",
        "\n",
        "The cost function used for logistic regression is the average of the log loss across all training examples:\n",
        "\n",
        "$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)}\\log (\\sigma(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-\\sigma(z(\\theta)^{(i)}))\\tag{5} $$\n",
        "* $m$ is the number of training examples\n",
        "* $y^{(i)}$ is the actual label of the i-th training example.\n",
        "* $h(z(\\theta)^{(i)})$ is the model's prediction for the i-th training example.\n",
        "\n",
        "The loss function for a single training example is\n",
        "$$ Loss = -1 \\times \\left( y^{(i)}\\log (\\sigma (z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-\\sigma(z(\\theta)^{(i)})) \\right)$$\n",
        "\n",
        "* All the $h$ values are between 0 and 1, so the logs will be negative. That is the reason for the factor of -1 applied to the sum of the two loss terms.\n",
        "* Note that when the model predicts 1 ($\\sigma(z(\\theta)) = 1$) and the label $y$ is also 1, the loss for that training example is 0.\n",
        "* Similarly, when the model predicts 0 ($\\sigma(z(\\theta)) = 0$) and the actual label is also 0, the loss for that training example is 0.\n",
        "* However, when the model prediction is close to 1 ($\\sigma(z(\\theta)) = 0.9999$) and the label is 0, the second term of the log loss becomes a large negative number, which is then multiplied by the overall factor of -1 to convert it to a positive loss value. $-1 \\times (1 - 0) \\times log(1 - 0.9999) \\approx 9.2$ The closer the model prediction gets to 1, the larger the loss.\n",
        "\n",
        "#### Update the weights\n",
        "\n",
        "To update your weight vector $\\theta$, you will apply gradient descent to iteratively improve your model's predictions.  \n",
        "The gradient of the cost function $J$ with respect to one of the weights $\\theta$ is:\n",
        "\n",
        "$$\\nabla J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m(\\sigma(z(\\theta)^{(i)})-y^{(i)})x_j \\tag{5}$$\n",
        "* 'i' is the index across all 'm' training examples.\n",
        "* 'j' is the index of the weight $\\theta_j$, so $x_j$ is the feature associated with weight $\\theta_j$\n",
        "\n",
        "* To update the weight $\\theta_j$, we adjust it by subtracting a fraction of the gradient determined by $\\alpha$:\n",
        "$$\\theta = \\theta - \\alpha \\times \\nabla_{\\theta}J(\\theta) $$\n",
        "* The learning rate $\\alpha$ is a value that we choose to control how big a single update will be."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-RFOXNt8gbIY"
      },
      "outputs": [],
      "source": [
        "## Where you see None, this means you have gotta do something there -- except __init__ function!!!!!\n",
        "class LR:\n",
        "    def __init__(self):\n",
        "        self._fitted_ = False\n",
        "        self.weight:np.ndarray = None\n",
        "\n",
        "    def fit(self, X, y, lr = 1e-10, max_iter = 1000):\n",
        "\n",
        "        if not self._fitted_:\n",
        "            n, m = X.shape\n",
        "            self.weight = 0.01*np.random.randn(m+1)\n",
        "            self._fitted_ = True\n",
        "\n",
        "        X_0=X.shape[0]\n",
        "        x_val_ones=[np.ones((X_0, 1)), X]\n",
        "        X_aug =  np.concatenate(x_val_ones, axis=1) ## add a column of ones from left!!!! like -- [ones, X] use np.concatenate\n",
        "\n",
        "        for j in range(max_iter):\n",
        "            ## get the loss --\n",
        "            ## in every 100 step print the loss (it should be decreasing over time)\n",
        "            ## get grads and update it below!!! see (5) above.\n",
        "            ## This is the point that you will hit the wall ---\n",
        "\n",
        "            log = self.get_logits(X)\n",
        "\n",
        "            log_pred = sigmoid(log)\n",
        "\n",
        "            err = log_pred - y\n",
        "\n",
        "            val= X_aug.T @ err\n",
        "\n",
        "            grads_val = val / len(y)\n",
        "\n",
        "            self.weight = self.weight - (lr * grads_val)\n",
        "\n",
        "            loss_val= y * np.log(log_pred) + (1 - y) * np.log(1 - log_pred)\n",
        "\n",
        "            loss_tot = -np.mean(loss_val) * len(y)\n",
        "\n",
        "            if j % 100 == 0:\n",
        "                print(loss_tot)\n",
        "\n",
        "    def get_logits(self, X):\n",
        "        assert self._fitted_, \"The model has not been fitted yet!\"\n",
        "        if len(X.shape) == 1:\n",
        "            X = np.expand_dims(X, axis = 0)\n",
        "        return np.concatenate([np.ones((X.shape[0], 1)), X], axis = 1) @ self.weight\n",
        "\n",
        "    def predict(self, X):\n",
        "        ## Get predictions here --- output array should be containing labels (only 0s and 1s)\n",
        "        ## hint: np.where\n",
        "        log = self.get_logits(X)\n",
        "        prob_val = sigmoid(log)\n",
        "        pred_result=np.where(prob_val >= 0.5, 1, 0)\n",
        "        return pred_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JE7hWTaygbIZ",
        "outputId": "0e0ee6c8-0554-4b14-85f6-fbf2a9d9394f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "693.1462797411714\n",
            "693.1235240431618\n",
            "693.100883302815\n",
            "693.0783569352282\n",
            "693.0559443584558\n",
            "693.0336449934975\n",
            "693.0114582642825\n",
            "692.9893835976551\n",
            "692.967420423361\n",
            "692.9455681740333\n",
            "692.9238262851777\n",
            "692.9021941951596\n",
            "692.880671345189\n",
            "692.8592571793067\n",
            "692.8379511443716\n",
            "692.8167526900455\n",
            "692.7956612687802\n",
            "692.7746763358036\n",
            "692.7537973491062\n",
            "692.7330237694271\n",
            "692.7123550602412\n",
            "692.6917906877453\n",
            "692.6713301208449\n",
            "692.650972831141\n",
            "692.6307182929162\n",
            "692.6105659831226\n",
            "692.5905153813676\n",
            "692.5705659699015\n",
            "692.5507172336042\n",
            "692.5309686599726\n",
            "692.5113197391067\n",
            "692.4917699636981\n",
            "692.4723188290161\n",
            "692.4529658328956\n",
            "692.4337104757244\n",
            "692.4145522604301\n",
            "692.3954906924682\n",
            "692.3765252798089\n",
            "692.357655532925\n",
            "692.3388809647797\n",
            "692.3202010908141\n",
            "692.3016154289345\n",
            "692.283123499501\n",
            "692.2647248253146\n",
            "692.2464189316053\n",
            "692.2282053460201\n",
            "692.2100835986113\n",
            "692.1920532218239\n",
            "692.1741137504839\n",
            "692.156264721787\n",
            "692.138505675286\n",
            "692.1208361528796\n",
            "692.1032556988007\n",
            "692.0857638596044\n",
            "692.0683601841569\n",
            "692.0510442236233\n",
            "692.0338155314571\n",
            "692.0166736633878\n",
            "691.9996181774103\n",
            "691.9826486337731\n",
            "691.9657645949674\n",
            "691.9489656257151\n",
            "691.9322512929591\n",
            "691.9156211658508\n",
            "691.8990748157394\n",
            "691.8826118161617\n",
            "691.86623174283\n",
            "691.8499341736222\n",
            "691.8337186885698\n",
            "691.8175848698482\n",
            "691.8015323017659\n",
            "691.7855605707525\n",
            "691.7696692653499\n",
            "691.7538579762003\n",
            "691.7381262960361\n",
            "691.7224738196695\n",
            "691.7069001439824\n",
            "691.6914048679149\n",
            "691.6759875924562\n",
            "691.6606479206332\n",
            "691.6453854575013\n",
            "691.6301998101336\n",
            "691.6150905876103\n",
            "691.6000574010097\n",
            "691.5850998633973\n",
            "691.5702175898157\n",
            "691.5554101972758\n",
            "691.5406773047453\n",
            "691.5260185331397\n",
            "691.5114335053125\n",
            "691.4969218460453\n",
            "691.4824831820379\n",
            "691.4681171418987\n",
            "691.4538233561352\n",
            "691.4396014571444\n",
            "691.425451079203\n",
            "691.411371858458\n",
            "691.3973634329175\n",
            "691.3834254424412\n",
            "691.3695575287306\n",
            "691.3557593353203\n",
            "691.3420305075681\n",
            "691.3283706926463\n",
            "691.3147795395323\n",
            "691.3012566989999\n",
            "691.2878018236087\n",
            "691.274414567697\n",
            "691.2610945873719\n",
            "691.2478415404998\n",
            "691.2346550866984\n",
            "691.2215348873272\n",
            "691.208480605479\n",
            "691.1954919059714\n",
            "691.1825684553368\n",
            "691.169709921815\n",
            "691.1569159753442\n",
            "691.144186287552\n",
            "691.1315205317471\n",
            "691.1189183829108\n",
            "691.1063795176881\n",
            "691.09390361438\n",
            "691.081490352934\n",
            "691.0691394149372\n",
            "691.0568504836065\n",
            "691.0446232437807\n",
            "691.0324573819133\n",
            "691.0203525860627\n",
            "691.0083085458849\n",
            "690.9963249526257\n",
            "690.9844014991115\n",
            "690.9725378797424\n",
            "690.9607337904832\n",
            "690.9489889288564\n",
            "690.9373029939331\n",
            "690.9256756863265\n",
            "690.9141067081827\n",
            "690.9025957631737\n",
            "690.8911425564895\n",
            "690.8797467948301\n",
            "690.868408186398\n",
            "690.8571264408904\n",
            "690.8459012694916\n",
            "690.8347323848657\n",
            "690.8236195011485\n",
            "690.8125623339404\n",
            "690.8015606002988\n",
            "690.7906140187308\n",
            "690.7797223091852\n",
            "690.768885193046\n",
            "690.7581023931245\n",
            "690.7473736336522\n",
            "690.7366986402732\n",
            "690.7260771400377\n",
            "690.7155088613938\n",
            "690.7049935341818\n",
            "690.6945308896252\n",
            "690.6841206603251\n",
            "690.6737625802527\n",
            "690.6634563847424\n",
            "690.6532018104842\n",
            "690.6429985955174\n",
            "690.6328464792236\n",
            "690.62274520232\n",
            "690.6126945068518\n",
            "690.6026941361863\n",
            "690.5927438350054\n",
            "690.5828433492998\n",
            "690.572992426361\n",
            "690.5631908147756\n",
            "690.5534382644184\n",
            "690.5437345264459\n",
            "690.5340793532893\n",
            "690.5244724986485\n",
            "690.5149137174856\n",
            "690.5054027660174\n",
            "690.4959394017108\n",
            "690.4865233832745\n",
            "690.4771544706538\n",
            "690.467832425024\n",
            "690.458557008784\n",
            "690.4493279855499\n",
            "690.440145120149\n",
            "690.4310081786134\n",
            "690.4219169281739\n",
            "690.4128711372539\n",
            "690.4038705754633\n",
            "690.3949150135918\n",
            "690.386004223604\n",
            "690.3771379786324\n",
            "690.3683160529716\n",
            "690.359538222073\n",
            "690.3508042625375\n",
            "690.342113952111\n",
            "690.3334670696778\n",
            "690.3248633952549\n",
            "690.3163027099857\n",
            "690.3077847961353\n",
            "690.2993094370836\n",
            "690.2908764173203\n",
            "690.2824855224391\n",
            "690.2741365391314\n",
            "690.2658292551814\n",
            "690.2575634594604\n",
            "690.2493389419205\n",
            "690.2411554935904\n",
            "690.2330129065681\n",
            "690.2249109740167\n",
            "690.2168494901587\n",
            "690.2088282502701\n",
            "690.2008470506757\n",
            "690.1929056887427\n",
            "690.1850039628764\n",
            "690.1771416725142\n",
            "690.16931861812\n",
            "690.1615346011802\n",
            "690.153789424197\n",
            "690.1460828906842\n",
            "690.138414805161\n",
            "690.1307849731479\n",
            "690.1231932011609\n",
            "690.1156392967064\n",
            "690.1081230682764\n",
            "690.100644325343\n",
            "690.0932028783538\n",
            "690.0857985387265\n",
            "690.0784311188443\n",
            "690.0711004320503\n",
            "690.0638062926436\n",
            "690.0565485158729\n",
            "690.0493269179331\n",
            "690.0421413159595\n",
            "690.034991528023\n",
            "690.027877373126\n",
            "690.0207986711969\n",
            "690.0137552430851\n",
            "690.0067469105575\n",
            "689.9997734962922\n",
            "689.9928348238748\n",
            "689.9859307177935\n",
            "689.9790610034347\n",
            "689.9722255070776\n",
            "689.9654240558906\n",
            "689.9586564779257\n",
            "689.9519226021148\n",
            "689.945222258265\n",
            "689.9385552770531\n",
            "689.9319214900229\n",
            "689.9253207295794\n",
            "689.9187528289845\n",
            "689.9122176223528\n",
            "689.9057149446478\n",
            "689.8992446316761\n",
            "689.8928065200845\n",
            "689.8864004473547\n",
            "689.8800262517997\n",
            "689.8736837725589\n",
            "689.867372849594\n",
            "689.861093323685\n",
            "689.854845036426\n",
            "689.8486278302207\n",
            "689.8424415482782\n",
            "689.8362860346093\n",
            "689.830161134022\n",
            "689.8240666921171\n",
            "689.818002555285\n",
            "689.8119685707012\n",
            "689.8059645863214\n",
            "689.799990450879\n",
            "689.7940460138798\n",
            "689.788131125599\n",
            "689.7822456370764\n",
            "689.776389400113\n",
            "689.7705622672668\n",
            "689.7647640918492\n",
            "689.7589947279207\n",
            "689.7532540302875\n",
            "689.7475418544971\n",
            "689.7418580568349\n",
            "689.7362024943202\n",
            "689.7305750247028\n",
            "689.7249755064588\n",
            "689.7194037987865\n",
            "689.713859761604\n",
            "689.7083432555437\n",
            "689.7028541419504\n",
            "689.6973922828762\n",
            "689.6919575410777\n",
            "689.686549780012\n",
            "689.6811688638331\n",
            "689.675814657389\n",
            "689.6704870262165\n",
            "689.6651858365394\n",
            "689.659910955264\n",
            "689.6546622499756\n",
            "689.6494395889356\n",
            "689.6442428410772\n",
            "689.6390718760027\n",
            "689.6339265639795\n",
            "689.6288067759367\n",
            "689.6237123834621\n",
            "689.6186432587987\n",
            "689.6135992748406\n",
            "689.6085803051307\n",
            "689.6035862238566\n",
            "689.5986169058478\n",
            "689.593672226572\n",
            "689.5887520621318\n",
            "689.5838562892618\n",
            "689.578984785325\n",
            "689.5741374283098\n",
            "689.569314096826\n",
            "689.5645146701031\n",
            "689.5597390279859\n",
            "689.5549870509317\n",
            "689.5502586200068\n",
            "689.5455536168843\n",
            "689.5408719238394\n",
            "689.5362134237483\n",
            "689.5315780000835\n",
            "689.5269655369111\n",
            "689.5223759188882\n",
            "689.5178090312595\n",
            "689.5132647598541\n",
            "689.5087429910833\n",
            "689.5042436119361\n",
            "689.4997665099779\n",
            "689.4953115733465\n",
            "689.4908786907496\n",
            "689.4864677514611\n",
            "689.4820786453193\n",
            "689.4777112627235\n",
            "689.4733654946306\n",
            "689.4690412325529\n",
            "689.4647383685551\n",
            "689.4604567952514\n",
            "689.4561964058024\n",
            "689.4519570939128\n",
            "689.4477387538282\n",
            "689.4435412803323\n",
            "689.4393645687443\n",
            "689.4352085149164\n",
            "689.4310730152304\n",
            "689.4269579665953\n",
            "689.422863266445\n",
            "689.4187888127349\n",
            "689.4147345039395\n",
            "689.4107002390498\n",
            "689.4066859175705\n",
            "689.4026914395176\n",
            "689.3987167054158\n",
            "689.394761616295\n",
            "689.3908260736889\n",
            "689.3869099796317\n",
            "689.3830132366559\n",
            "689.3791357477893\n",
            "689.3752774165528\n",
            "689.3714381469576\n",
            "689.367617843503\n",
            "689.3638164111738\n",
            "689.3600337554375\n",
            "689.3562697822418\n",
            "689.3525243980131\n",
            "689.3487975096524\n",
            "689.3450890245343\n",
            "689.3413988505039\n",
            "689.3377268958743\n",
            "689.3340730694247\n",
            "689.3304372803973\n",
            "689.3268194384955\n",
            "689.3232194538812\n",
            "689.3196372371729\n",
            "689.3160726994423\n",
            "689.3125257522136\n",
            "689.3089963074591\n",
            "689.3054842775992\n",
            "689.3019895754984\n",
            "689.2985121144632\n",
            "689.2950518082408\n",
            "689.2916085710159\n",
            "689.2881823174085\n",
            "689.2847729624726\n",
            "689.2813804216926\n",
            "689.2780046109821\n",
            "689.2746454466811\n",
            "689.2713028455546\n",
            "689.2679767247893\n",
            "689.2646670019922\n",
            "689.2613735951884\n",
            "689.2580964228187\n",
            "689.2548354037374\n",
            "689.2515904572106\n",
            "689.2483615029136\n",
            "689.2451484609289\n",
            "689.241951251745\n",
            "689.2387697962524\n",
            "689.2356040157433\n",
            "689.2324538319092\n",
            "689.2293191668375\n",
            "689.2261999430116\n",
            "689.2230960833074\n",
            "689.2200075109911\n",
            "689.2169341497188\n",
            "689.2138759235324\n",
            "689.2108327568596\n",
            "689.2078045745104\n",
            "689.2047913016756\n",
            "689.2017928639257\n",
            "689.1988091872073\n",
            "689.1958401978427\n",
            "689.1928858225272\n",
            "689.189945988327\n",
            "689.1870206226783\n",
            "689.1841096533841\n",
            "689.1812130086133\n",
            "689.1783306168984\n",
            "689.1754624071335\n",
            "689.1726083085729\n",
            "689.1697682508288\n",
            "689.1669421638699\n",
            "689.1641299780189\n",
            "689.1613316239516\n",
            "689.1585470326942\n",
            "689.1557761356221\n",
            "689.1530188644579\n",
            "689.1502751512694\n",
            "689.1475449284683\n",
            "689.1448281288083\n",
            "689.1421246853829\n",
            "689.1394345316239\n",
            "689.1367576013001\n",
            "689.1340938285152\n",
            "689.131443147706\n",
            "689.1288054936404\n",
            "689.1261808014168\n",
            "689.1235690064614\n",
            "689.1209700445264\n",
            "689.1183838516895\n",
            "689.1158103643509\n",
            "689.1132495192325\n",
            "689.1107012533759\n",
            "689.1081655041406\n",
            "689.1056422092032\n",
            "689.1031313065546\n",
            "689.100632734499\n",
            "689.0981464316527\n",
            "689.0956723369418\n",
            "689.0932103896005\n",
            "689.0907605291704\n",
            "689.0883226954986\n",
            "689.0858968287353\n",
            "689.0834828693331\n",
            "689.0810807580457\n",
            "689.0786904359253\n",
            "689.0763118443219\n",
            "689.073944924882\n",
            "689.0715896195459\n",
            "689.0692458705472\n",
            "689.0669136204111\n",
            "689.0645928119532\n",
            "689.0622833882767\n",
            "689.0599852927727\n",
            "689.0576984691177\n",
            "689.0554228612721\n",
            "689.0531584134792\n",
            "689.0509050702633\n",
            "689.0486627764291\n",
            "689.0464314770588\n",
            "689.0442111175123\n",
            "689.0420016434243\n",
            "689.0398030007043\n",
            "689.0376151355342\n",
            "689.0354379943672\n",
            "689.0332715239263\n",
            "689.0311156712032\n",
            "689.028970383457\n",
            "689.026835608212\n",
            "689.0247112932575\n",
            "689.0225973866457\n",
            "689.0204938366904\n",
            "689.0184005919662\n",
            "689.0163176013062\n",
            "689.0142448138018\n",
            "689.0121821788005\n",
            "689.0101296459048\n",
            "689.0080871649716\n",
            "689.0060546861096\n",
            "689.0040321596794\n",
            "689.002019536291\n",
            "689.0000167668032\n",
            "688.9980238023225\n",
            "688.9960405942012\n",
            "688.9940670940366\n",
            "688.9921032536695\n",
            "688.9901490251833\n",
            "688.9882043609025\n",
            "688.9862692133911\n",
            "688.9843435354524\n",
            "688.9824272801266\n",
            "688.9805204006907\n",
            "688.9786228506564\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-03d351bba57f>:41: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  float(lr.predict(np.random.randn(10))) ## <- Does it throw an error ?\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "##Let's give it a try --- probably you will spend some time here !!!!\n",
        "np.random.seed(0)\n",
        "X = np.random.randn(1000, 10)\n",
        "y = np.random.randint(0, 2, 1000)\n",
        "\n",
        "lr = LR()\n",
        "lr.fit(X, y, lr = 1e-4, max_iter = 50000)\n",
        "\"\"\" You should see something like as follows\n",
        "693.1462797411714\n",
        "693.1235240431618\n",
        "693.1008833028151\n",
        "693.078356935228\n",
        "693.0559443584558\n",
        "693.0336449934978\n",
        "693.0114582642825\n",
        "692.9893835976551\n",
        "692.967420423361\n",
        "692.9455681740333\n",
        "692.9238262851777\n",
        "692.9021941951596\n",
        "692.8806713451888\n",
        "692.8592571793067\n",
        "692.8379511443716\n",
        "692.8167526900455\n",
        "692.7956612687802\n",
        "692.7746763358036\n",
        "692.7537973491062\n",
        "692.7330237694271\n",
        "692.7123550602412\n",
        "692.6917906877453\n",
        "692.6713301208449\n",
        "692.650972831141\n",
        "692.6307182929162\n",
        "...\n",
        "688.9843435354524\n",
        "688.9824272801267\n",
        "688.9805204006907\n",
        "688.9786228506564\n",
        "\"\"\"\n",
        "sum(lr.predict(np.random.randn(100,10)))/100  ## <-- is this number close to 0.5 ?\n",
        "float(lr.predict(np.random.randn(10))) ## <- Does it throw an error ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J23RmAJgbIZ"
      },
      "source": [
        "### Part 2: --- AncientGPT ---\n",
        "### You are now ready to code a very old but still efficient tweet classifier! Import functions and data. We shall be using nltk package (use !pip install nlk if you receive any error)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GwV1EVYggbIZ",
        "outputId": "b923dc58-df71-4ccc-8de0-48272974c031",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# run this cell to import nltk and some other files!!!\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from os import getcwd\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import sklearn\n",
        "\n",
        "filePath = f\"{getcwd()}/../tmp2/\"\n",
        "nltk.data.path.append(filePath)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6aPy_UngbIa"
      },
      "source": [
        "### Read the tweets using pd.read_csv, and browse them.\n",
        "### 1) Read the csv file,\n",
        "### 2) Grab the columns, X and y\n",
        "### 3) using sklearn.model_selection.train_test split split the dataset into two parts\n",
        "### 3.5) names should be for compatibility train_x, train_y, test_x, test_y\n",
        "### 4) Convert everything to numpy arrays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "LvDgiUMZgbIa"
      },
      "outputs": [],
      "source": [
        "##Your code here ##\n",
        "## Start by reading csv file -- download it directly from github, download it locally!!!\n",
        "## split it into train test split!!!\n",
        "## make sure that the names are train_x, test_x, train_y, test_y\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "url=\"https://raw.githubusercontent.com/y-akbal/ADA440_Python_4_DS/main/ALE/tweets.csv\"\n",
        "df = pd.read_csv(url)\n",
        "train_x, train_y, test_x, test_y = train_test_split(df['Tweets'], df['Sentiment'], test_size=0.2, random_state=42)\n",
        "\n",
        "train_x=np.array(train_x)\n",
        "train_y=np.array(train_y)\n",
        "test_x=np.array(test_x)\n",
        "test_y=np.array(test_y)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DWUd10xgbIa"
      },
      "source": [
        "### Below you shall create some helper functions process_tweet and build_freqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YKhFgY0sgbIb"
      },
      "outputs": [],
      "source": [
        "def process_tweet(tweet:str)->list[str]:\n",
        "    \"\"\"Process tweet function.\n",
        "    Input:\n",
        "        tweet: a string containing a tweet\n",
        "    Output:\n",
        "        tweets_clean: a list of words containing the processed tweet\n",
        "    \"\"\"\n",
        "\n",
        "    \"\"\"\n",
        "    google the following see what they do???\n",
        "\n",
        "    from nltk.corpus import stopwords\n",
        "    from nltk.stem import PorterStemmer\n",
        "    from nltk.tokenize import TweetTokenizer  -- (use with setup preserve_case=False, strip_handles=True,\n",
        "                               reduce_len=True)\n",
        "    \"\"\"\n",
        "\n",
        "    stemmer = PorterStemmer() ##Initalize stemmer\n",
        "    stopwords_english = stopwords.words('english') ## Grab the stop_words\n",
        "    ## --- ##\n",
        "    ## Below you will right regex -- feel free to use ChatGPT!!!\n",
        "    ## It should be of the following form\n",
        "    ## tweet = re.sub(r\" --pattern--\", \"\", tweet)\n",
        "    # Steps -- begining of the regex stuff\n",
        "    # remove stock market tickers like $GE\n",
        "    # remove old style retweet text \"RT\"\n",
        "    # remove hyperlinks\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    # end of regex stuff\n",
        "\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "    tweet = re.sub(r'https?:\\/\\/\\S+', '', tweet)\n",
        "    tweet = re.sub(r'#', '', tweet)\n",
        "\n",
        "\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                               reduce_len=True)\n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "    ## Below you will get tokenized tweet!!!\n",
        "    tweets_clean = []\n",
        "    for word in tweet_tokens:\n",
        "        if (word not in stopwords_english and  # remove stopwords\n",
        "                word not in string.punctuation):  # remove punctuation\n",
        "            stem_word = stemmer.stem(word)  # stemming word\n",
        "            tweets_clean.append(stem_word)\n",
        "\n",
        "    return tweets_clean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6pvIeCnBgbIb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8851db9f-4343-4aea-86ca-d2556e6a5e3c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['yep', 'yep', 'hello', 'dude', 'doin']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "process_tweet(\"Yep yep Hello dude, how you doin?\") ## should be == ['yep', 'yep', 'hello', 'dude', 'doin']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyV214rggbIc"
      },
      "source": [
        "### Process tweet\n",
        "The given function process_tweet() tokenizes the tweet into individual words, removes stop words and applies stemming."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WHXgCq_hgbIc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f7bcd05-914f-4904-b01b-38bf2d267518"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is an example of a positive tweet: \n",
            " http://t.co/ziJiJYLDXT via @youtube...Reality is!! :(\n",
            "\n",
            "This is an example of the processed version of the tweet: \n",
            " ['via', '...', 'realiti', ':(']\n"
          ]
        }
      ],
      "source": [
        "# test the function below\n",
        "print('This is an example of a positive tweet: \\n', train_x[55])\n",
        "print('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(train_x[55]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBihC_EmgbIc"
      },
      "source": [
        "#### Expected output should more or less as follows!\n",
        "```\n",
        "This is an example of a positive tweet:\n",
        " #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
        "\n",
        "This is an example of the processes version:\n",
        " ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "x_j_92NGgbId"
      },
      "outputs": [],
      "source": [
        "def build_freqs(tweets, ys):\n",
        "    \"\"\"Build frequencies.\n",
        "    Input:\n",
        "        tweets: a list of tweets\n",
        "        ys: an m x 1 array with the sentiment label of each tweet\n",
        "            (either 0 or 1)\n",
        "    Output:\n",
        "        freqs: a dictionary mapping each (word, sentiment) pair to its\n",
        "        frequency\n",
        "    \"\"\"\n",
        "    # Convert np array to list since zip needs an iterable.\n",
        "    # The squeeze is necessary or the list ends up with one element.\n",
        "    # Also note that this is just a NOP if ys is already a list.\n",
        "    yslist = np.squeeze(ys).tolist()\n",
        "\n",
        "    # Start with an empty dictionary and populate it by looping over all tweets\n",
        "    # and over all processed words in each tweet.\n",
        "    freqs = {}\n",
        "    #\n",
        "    for y, tweet in zip(yslist, tweets):\n",
        "        for word in process_tweet(tweet):\n",
        "            pair = (word, y)\n",
        "            if pair in freqs:\n",
        "                freqs[pair] += 1\n",
        "            else:\n",
        "                freqs[pair] = 1\n",
        "\n",
        "    return freqs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "8mxvcaZBgbId",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ba1d911-13ca-4c9c-c488-9047cf73d1fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "type(freqs) = <class 'dict'>\n",
            "len(freqs) = 13142\n"
          ]
        }
      ],
      "source": [
        "# create frequency dictionary\n",
        "freqs = build_freqs(train_x, train_y)\n",
        "# check the output\n",
        "print(\"type(freqs) = \" + str(type(freqs)))\n",
        "print(\"len(freqs) = \" + str(len(freqs.keys())))\n",
        "## What do you see here???\n",
        "## See what freqs contains????"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gYZtZKAgbId"
      },
      "source": [
        "## Part 3: Extracting the features\n",
        "\n",
        "* Given a list of tweets, extract the features and store them in a matrix. You will extract two features.\n",
        "    * The first feature is the number of positive words in a tweet.\n",
        "    * The second feature is the number of negative words in a tweet.\n",
        "* Then train your logistic regression classifier on these features.\n",
        "* Test the classifier on a validation set.\n",
        "\n",
        "### Instructions: Implement the extract_features function.\n",
        "* This function takes in a single tweet.\n",
        "* Process the tweet using the imported process_tweet() function and save the list of tweet words.\n",
        "* Loop through each word in the list of processed words\n",
        "    * For each word, check the freqs dictionary for the count when that word has a positive '1' label. (Check for the key (word, 1.0)\n",
        "    * Do the same for the count for when the word is associated with the negative label '0'. (Check for the key (word, 0.0).)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "wrDb7g6rgbIe"
      },
      "outputs": [],
      "source": [
        "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "def extract_features(tweet, freqs):\n",
        "    '''\n",
        "    Input:\n",
        "        tweet: a list of words for one tweet\n",
        "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
        "    Output:\n",
        "        x: a feature vector of dimension (1,3)\n",
        "    '''\n",
        "    # process_tweet tokenizes, stems, and removes stopwords\n",
        "    word_l = process_tweet(tweet)\n",
        "\n",
        "    # 3 elements in the form of a 1 x 2 vector\n",
        "    x = np.zeros((1, 2))\n",
        "\n",
        "\n",
        "    # loop through each word in the list of words\n",
        "    for word in word_l:\n",
        "\n",
        "        # increment the word count for the positive label 1\n",
        "        x[0,0] += freqs.get((word, 1.0), 0)\n",
        "        # increment the word count for the negative label 0\n",
        "        x[0,1] += freqs.get((word, 0), 0)\n",
        "\n",
        "    ### END CODE HERE ###\n",
        "    assert(x.shape == (1, 2))\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9twWadlNgbIe"
      },
      "source": [
        "# Extract Features\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZclD_LZgbIe"
      },
      "source": [
        "## Part 3: Training Your Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Ri3mYrS7gbIf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01e548e2-883e-4f61-f0fd-fff9a4f84c0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5548.742054854694\n",
            "5548.563486080486\n",
            "5548.384948759359\n",
            "5548.206442779534\n",
            "5548.027968029795\n",
            "5547.849524399486\n",
            "5547.6711117785\n",
            "5547.492730057287\n",
            "5547.3143791268485\n",
            "5547.136058878727\n",
            "5546.957769205013\n",
            "5546.779509998339\n",
            "5546.601281151872\n",
            "5546.423082559319\n",
            "5546.244914114917\n",
            "5546.066775713438\n",
            "5545.888667250176\n",
            "5545.710588620955\n",
            "5545.532539722118\n",
            "5545.35452045053\n",
            "5545.176530703573\n",
            "5544.998570379144\n",
            "5544.820639375652\n",
            "5544.642737592012\n",
            "5544.464864927652\n",
            "5544.2870212825\n",
            "5544.109206556987\n",
            "5543.931420652045\n",
            "5543.7536634691\n",
            "5543.575934910073\n",
            "5543.398234877377\n",
            "5543.220563273917\n",
            "5543.042920003081\n",
            "5542.865304968744\n",
            "5542.68771807526\n",
            "5542.510159227466\n",
            "5542.332628330673\n",
            "5542.15512529067\n",
            "5541.977650013718\n",
            "5541.800202406543\n",
            "5541.622782376343\n",
            "5541.445389830783\n",
            "5541.2680246779855\n",
            "5541.090686826537\n",
            "5540.913376185483\n",
            "5540.736092664321\n",
            "5540.558836173008\n",
            "5540.381606621946\n",
            "5540.20440392199\n",
            "5540.027227984441\n",
            "5539.850078721045\n",
            "5539.672956043989\n",
            "5539.4958598659005\n",
            "5539.318790099846\n",
            "5539.141746659327\n",
            "5538.964729458277\n",
            "5538.787738411063\n",
            "5538.6107734324805\n",
            "5538.433834437751\n",
            "5538.256921342522\n",
            "5538.080034062863\n",
            "5537.9031725152645\n",
            "5537.726336616635\n",
            "5537.5495262843\n",
            "5537.3727414359955\n",
            "5537.1959819898775\n",
            "5537.019247864504\n",
            "5536.8425389788445\n",
            "5536.665855252275\n",
            "5536.489196604575\n",
            "5536.312562955925\n",
            "5536.135954226906\n",
            "5535.959370338496\n",
            "5535.782811212069\n",
            "5535.606276769395\n",
            "5535.4297669326315\n",
            "5535.253281624329\n",
            "5535.076820767426\n",
            "5534.900384285247\n",
            "5534.723972101498\n",
            "5534.5475841402695\n",
            "5534.371220326031\n",
            "5534.194880583631\n",
            "5534.018564838292\n",
            "5533.842273015614\n",
            "5533.66600504157\n",
            "5533.4897608425\n",
            "5533.313540345114\n",
            "5533.137343476492\n",
            "5532.961170164075\n",
            "5532.785020335669\n",
            "5532.6088939194415\n",
            "5532.432790843921\n",
            "5532.256711037988\n",
            "5532.080654430887\n",
            "5531.904620952213\n",
            "5531.72861053191\n",
            "5531.552623100278\n",
            "5531.376658587964\n",
            "5531.2007169259605\n",
            "5531.024798045608\n",
            "5530.848901878586\n",
            "5530.673028356923\n",
            "5530.497177412983\n",
            "5530.321348979467\n",
            "5530.145542989416\n",
            "5529.969759376205\n",
            "5529.793998073543\n",
            "5529.618259015468\n",
            "5529.44254213635\n",
            "5529.26684737089\n",
            "5529.091174654111\n",
            "5528.915523921361\n",
            "5528.739895108316\n",
            "5528.56428815097\n",
            "5528.38870298564\n",
            "5528.213139548956\n",
            "5528.03759777787\n",
            "5527.86207760965\n",
            "5527.686578981875\n",
            "5527.511101832435\n",
            "5527.335646099535\n",
            "5527.160211721683\n",
            "5526.984798637701\n",
            "5526.809406786711\n",
            "5526.634036108146\n",
            "5526.458686541733\n",
            "5526.283358027509\n",
            "5526.108050505805\n",
            "5525.932763917255\n",
            "5525.7574982027845\n",
            "5525.582253303618\n",
            "5525.407029161272\n",
            "5525.231825717557\n",
            "5525.056642914571\n",
            "5524.881480694707\n",
            "5524.706339000641\n",
            "5524.531217775337\n",
            "5524.3561169620425\n",
            "5524.181036504293\n",
            "5524.005976345901\n",
            "5523.8309364309625\n",
            "5523.655916703854\n",
            "5523.480917109227\n",
            "5523.30593759201\n",
            "5523.130978097408\n",
            "5522.956038570899\n",
            "5522.7811189582335\n",
            "5522.606219205433\n",
            "5522.431339258788\n",
            "5522.2564790648585\n",
            "5522.08163857047\n",
            "5521.906817722715\n",
            "5521.732016468948\n",
            "5521.557234756789\n",
            "5521.3824725341165\n",
            "5521.207729749073\n",
            "5521.033006350057\n",
            "5520.858302285727\n",
            "5520.683617504995\n",
            "5520.508951957029\n",
            "5520.334305591253\n",
            "5520.15967835734\n",
            "5519.985070205217\n",
            "5519.810481085058\n",
            "5519.63591094729\n",
            "5519.461359742583\n",
            "5519.2868274218545\n",
            "5519.11231393627\n",
            "5518.937819237233\n",
            "5518.763343276396\n",
            "5518.5888860056475\n",
            "5518.414447377117\n",
            "5518.240027343174\n",
            "5518.065625856427\n",
            "5517.891242869717\n",
            "5517.716878336123\n",
            "5517.542532208959\n",
            "5517.368204441768\n",
            "5517.193894988329\n",
            "5517.019603802647\n",
            "5516.845330838962\n",
            "5516.671076051738\n",
            "5516.496839395664\n",
            "5516.322620825662\n",
            "5516.148420296874\n",
            "5515.974237764665\n",
            "5515.800073184624\n",
            "5515.625926512563\n",
            "5515.451797704511\n",
            "5515.277686716717\n",
            "5515.10359350565\n",
            "5514.929518027995\n",
            "5514.755460240651\n",
            "5514.581420100734\n",
            "5514.407397565574\n",
            "5514.233392592712\n",
            "5514.0594051399\n",
            "5513.885435165102\n",
            "5513.711482626493\n",
            "5513.537547482452\n",
            "5513.363629691568\n",
            "5513.189729212636\n",
            "5513.015846004658\n",
            "5512.841980026836\n",
            "5512.668131238577\n",
            "5512.494299599495\n",
            "5512.320485069395\n",
            "5512.14668760829\n",
            "5511.972907176392\n",
            "5511.799143734107\n",
            "5511.62539724204\n",
            "5511.451667660993\n",
            "5511.27795495196\n",
            "5511.104259076135\n",
            "5510.9305799949\n",
            "5510.756917669828\n",
            "5510.5832720626895\n",
            "5510.409643135441\n",
            "5510.2360308502275\n",
            "5510.062435169385\n",
            "5509.888856055434\n",
            "5509.715293471086\n",
            "5509.541747379231\n",
            "5509.368217742951\n",
            "5509.194704525507\n",
            "5509.021207690343\n",
            "5508.847727201087\n",
            "5508.674263021546\n",
            "5508.500815115707\n",
            "5508.327383447738\n",
            "5508.153967981981\n",
            "5507.98056868296\n",
            "5507.807185515372\n",
            "5507.633818444092\n",
            "5507.460467434168\n",
            "5507.287132450821\n",
            "5507.113813459448\n",
            "5506.940510425611\n",
            "5506.767223315053\n",
            "5506.593952093679\n",
            "5506.420696727568\n",
            "5506.247457182964\n",
            "5506.074233426282\n",
            "5505.901025424102\n",
            "5505.72783314317\n",
            "5505.554656550397\n",
            "5505.381495612861\n",
            "5505.2083502977985\n",
            "5505.035220572612\n",
            "5504.8621064048675\n",
            "5504.689007762286\n",
            "5504.515924612753\n",
            "5504.342856924315\n",
            "5504.169804665175\n",
            "5503.996767803691\n",
            "5503.823746308382\n",
            "5503.650740147923\n",
            "5503.47774929114\n",
            "5503.304773707019\n",
            "5503.131813364699\n",
            "5502.95886823347\n",
            "5502.785938282774\n",
            "5502.613023482204\n",
            "5502.440123801509\n",
            "5502.267239210583\n",
            "5502.094369679471\n",
            "5501.921515178365\n",
            "5501.748675677607\n",
            "5501.575851147685\n",
            "5501.4030415592315\n",
            "5501.2302468830285\n",
            "5501.057467089999\n",
            "5500.884702151212\n",
            "5500.71195203788\n",
            "5500.539216721356\n",
            "5500.366496173138\n",
            "5500.193790364863\n",
            "5500.02109926831\n",
            "5499.848422855393\n",
            "5499.675761098175\n",
            "5499.503113968846\n",
            "5499.330481439742\n",
            "5499.15786348333\n",
            "5498.985260072221\n",
            "5498.812671179151\n",
            "5498.640096777\n",
            "5498.467536838778\n",
            "5498.294991337629\n",
            "5498.122460246831\n",
            "5497.949943539792\n",
            "5497.7774411900555\n",
            "5497.6049531712915\n",
            "5497.432479457303\n",
            "5497.26002002202\n",
            "5497.0875748395065\n",
            "5496.915143883947\n",
            "5496.74272712966\n",
            "5496.5703245510895\n",
            "5496.397936122803\n",
            "5496.225561819499\n",
            "5496.053201615994\n",
            "5495.880855487236\n",
            "5495.708523408293\n",
            "5495.536205354356\n",
            "5495.363901300738\n",
            "5495.191611222879\n",
            "5495.019335096333\n",
            "5494.84707289678\n",
            "5494.674824600019\n",
            "5494.502590181966\n",
            "5494.330369618659\n",
            "5494.158162886252\n",
            "5493.985969961019\n",
            "5493.813790819349\n",
            "5493.641625437749\n",
            "5493.46947379284\n",
            "5493.297335861361\n",
            "5493.125211620163\n",
            "5492.953101046213\n",
            "5492.781004116592\n",
            "5492.608920808491\n",
            "5492.436851099219\n",
            "5492.26479496619\n",
            "5492.092752386934\n",
            "5491.92072333909\n",
            "5491.74870780041\n",
            "5491.576705748751\n",
            "5491.404717162082\n",
            "5491.232742018481\n",
            "5491.060780296131\n",
            "5490.888831973328\n",
            "5490.716897028468\n",
            "5490.544975440059\n",
            "5490.373067186711\n",
            "5490.201172247141\n",
            "5490.0292906001705\n",
            "5489.857422224728\n",
            "5489.68556709984\n",
            "5489.513725204641\n",
            "5489.341896518366\n",
            "5489.170081020353\n",
            "5488.998278690041\n",
            "5488.826489506971\n",
            "5488.654713450782\n",
            "5488.482950501218\n",
            "5488.311200638118\n",
            "5488.139463841422\n",
            "5487.967740091169\n",
            "5487.796029367495\n",
            "5487.624331650635\n",
            "5487.4526469209195\n",
            "5487.280975158777\n",
            "5487.109316344731\n",
            "5486.9376704594015\n",
            "5486.766037483504\n",
            "5486.594417397847\n",
            "5486.422810183335\n",
            "5486.251215820966\n",
            "5486.079634291829\n",
            "5485.90806557711\n",
            "5485.736509658085\n",
            "5485.564966516119\n",
            "5485.393436132673\n",
            "5485.221918489297\n",
            "5485.050413567633\n",
            "5484.878921349409\n",
            "5484.7074418164475\n",
            "5484.535974950657\n",
            "5484.364520734035\n",
            "5484.193079148668\n",
            "5484.021650176732\n",
            "5483.8502338004855\n",
            "5483.678830002278\n",
            "5483.507438764545\n",
            "5483.336060069807\n",
            "5483.164693900669\n",
            "5482.993340239824\n",
            "5482.821999070047\n",
            "5482.650670374199\n",
            "5482.479354135225\n",
            "5482.3080503361525\n",
            "5482.136758960092\n",
            "5481.965479990237\n",
            "5481.794213409865\n",
            "5481.62295920233\n",
            "5481.451717351073\n",
            "5481.280487839615\n",
            "5481.109270651555\n",
            "5480.9380657705715\n",
            "5480.766873180428\n",
            "5480.595692864962\n",
            "5480.424524808093\n",
            "5480.253368993817\n",
            "5480.082225406211\n",
            "5479.911094029427\n",
            "5479.739974847695\n",
            "5479.568867845322\n",
            "5479.397773006693\n",
            "5479.226690316265\n",
            "5479.055619758578\n",
            "5478.88456131824\n",
            "5478.713514979941\n",
            "5478.542480728438\n",
            "5478.371458548568\n",
            "5478.200448425239\n",
            "5478.029450343436\n",
            "5477.858464288214\n",
            "5477.6874902447\n",
            "5477.516528198097\n",
            "5477.345578133678\n",
            "5477.1746400367865\n",
            "5477.003713892841\n",
            "5476.832799687327\n",
            "5476.661897405803\n",
            "5476.4910070338965\n",
            "5476.320128557305\n",
            "5476.149261961798\n",
            "5475.978407233212\n",
            "5475.807564357452\n",
            "5475.636733320493\n",
            "5475.465914108374\n",
            "5475.29510670721\n",
            "5475.124311103176\n",
            "5474.953527282518\n",
            "5474.782755231547\n",
            "5474.611994936639\n",
            "5474.441246384242\n",
            "5474.2705095608635\n",
            "5474.099784453081\n",
            "5473.9290710475325\n",
            "5473.758369330926\n",
            "5473.58767929003\n",
            "5473.417000911679\n",
            "5473.2463341827715\n",
            "5473.075679090267\n",
            "5472.905035621192\n",
            "5472.7344037626335\n",
            "5472.5637835017405\n",
            "5472.393174825727\n",
            "5472.222577721868\n",
            "5472.051992177496\n",
            "5471.881418180011\n",
            "5471.710855716869\n",
            "5471.540304775591\n",
            "5471.369765343754\n",
            "5471.199237409\n",
            "5471.028720959024\n",
            "5470.858215981589\n",
            "5470.68772246451\n",
            "5470.517240395664\n",
            "5470.346769762986\n",
            "5470.176310554471\n",
            "5470.0058627581675\n",
            "5469.835426362188\n",
            "5469.665001354695\n",
            "5469.494587723917\n",
            "5469.324185458129\n",
            "5469.153794545672\n",
            "5468.983414974939\n",
            "5468.813046734377\n",
            "5468.642689812493\n",
            "5468.472344197846\n",
            "5468.302009879051\n",
            "5468.13168684478\n",
            "5467.961375083758\n",
            "5467.791074584761\n",
            "5467.620785336625\n",
            "5467.450507328236\n",
            "5467.280240548535\n",
            "5467.109984986516\n",
            "5466.939740631225\n",
            "5466.7695074717585\n",
            "5466.599285497272\n",
            "5466.429074696967\n",
            "5466.258875060102\n",
            "5466.08868657598\n",
            "5465.918509233964\n",
            "5465.748343023461\n",
            "5465.578187933931\n",
            "5465.408043954888\n",
            "5465.237911075892\n",
            "5465.067789286553\n",
            "5464.897678576535\n",
            "5464.727578935548\n",
            "5464.5574903533525\n",
            "5464.387412819757\n",
            "5464.21734632462\n",
            "5464.04729085785\n",
            "5463.877246409398\n",
            "5463.707212969273\n",
            "5463.5371905275215\n",
            "5463.367179074244\n",
            "5463.197178599585\n",
            "5463.02718909374\n",
            "5462.8572105469475\n",
            "5462.687242949494\n",
            "5462.517286291715\n",
            "5462.347340563986\n",
            "5462.177405756736\n",
            "accuracy: 99.05%\n"
          ]
        }
      ],
      "source": [
        "## Here you are on your own no instructions!!!!\n",
        "## You will need to convert train_x to train_x with features extracted\n",
        "## Normalize your data -- with min-max normalization ---\n",
        "## create lr = LR(), call lr.fit ----  usual stuff!!!\n",
        "## Normalize train_x, and train\n",
        "## What accuracy you get on test set?\n",
        "\n",
        "\n",
        "df = pd.read_csv(url)\n",
        "train_x, test_x, train_y, test_y = train_test_split(df['Tweets'], df['Sentiment'], test_size=0.2, random_state=42)\n",
        "train_x = np.array(train_x)\n",
        "train_y = np.array(train_y)\n",
        "test_x = np.array(test_x)\n",
        "test_y = np.array(test_y)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def min_max_normalize(X):\n",
        "    X_min = X.min(axis=0)\n",
        "    X_max = X.max(axis=0)\n",
        "    X_norm = (X - X_min) / (X_max - X_min)\n",
        "    return X_norm\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def extract_features_(tweets):\n",
        "    freqs = build_freqs(train_x, train_y)\n",
        "    n=len(tweets)\n",
        "    X_val = np.zeros((n, 2))\n",
        "    for i, tweet in enumerate(tweets):\n",
        "        X_val[i, :] = extract_features(tweet, freqs)\n",
        "    return X_val\n",
        "\n",
        "\n",
        "\n",
        "x_features_train = extract_features_(train_x)\n",
        "x_features_test = extract_features_(test_x)\n",
        "\n",
        "x_features_normalized_train= min_max_normalize(x_features_train)\n",
        "x_features_normalized_test = min_max_normalize(x_features_test)\n",
        "\n",
        "lr_model = LR()\n",
        "lr_model.fit(x_features_normalized_train, train_y, lr=1e-4, max_iter=50000)\n",
        "\n",
        "pred_test_ = lr_model.predict(x_features_normalized_test)\n",
        "\n",
        "\n",
        "accuracy = np.mean(pred_test_ == test_y)\n",
        "\n",
        "print(f\"accuracy: {accuracy * 100:.2f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "coursera": {
      "schema_names": [
        "NLPC1-1"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}